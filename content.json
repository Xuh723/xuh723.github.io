{"pages":[],"posts":[{"title":"利用hexo搭建github博客","text":"概述Hexo 是个人博客框架，利用 Hexo，一方面可以用各种各样的博客主题模板搭建好看的博客，另一方面可以快速生成静态网页。然后利用 github 的发布功能生成网站，就可以简单搭建自己的个人博客。 Hexo 的安装参考 Hexo 官方文档的说法，Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他标记语言）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 安装依赖Hexo 依赖 git 和 nodeJS 两个组件。 安装 git直接在 git 官网下载，按照说明进行安装，这里不再赘述。 安装 nodeJSnodeJS 官网，下载推荐的版本。注意需要和 Hexo 的版本匹配，一般建议直接安装最新的版本。另外安装的过程中需要安装 npm，方便后续安装包括 Hexo 在内的其他 nodeJS 组件。 安装 hexogit 和 nodeJS 安装完成之后，可以先进行校验。在命令行输入 git 命令和 node 命令，以及 npm 命令查看版本，观察是否已经成功安装。如果找不到命令需要添加安装的可执行文件到 PATH 环境变量中。 123git versionnode --versionnpm --version 安装完成之后，需要在 npm 中安装 hexo。建议在安装之前先指定一个目录作为 npm 的全局安装目录，方便收纳这种非单一项目的安装内容。 12npm config set prefix &quot;&lt;folder&gt;&quot;npm install -g hexo-cli 安装完成之后可以执行命令查看 Hexo 的版本，确认是否安装成功。 1hexo --version Hexo 初始化安装完成之后，需要选择一个文件夹作为我们的工作目录。 继续根据 Hexo 官方文档的的指引，选定目录之后对指定目录进行初始化。 123cd &lt;folder&gt;hexo initnpm install 执行完成之后工作目录就初始化完毕了，之后就可以正式开始博客的搭建。 选择主题在 Hexo 的主题列表中选择喜欢的博客主题，然后根据主题的介绍进行安装。不同的主题有不同的安装方式，一般在主题的 github 页面中都有详细的说明，可以根据说明进行安装。不同的主题的安装配置等都不一样，所以这里需要根据选择的主题相关的说明自行准备。 博客配置根据 Hexo 配置 文档的说明进行配置。主要是网站下的一些配置，例如 title，description，author 等。有些主题也有配置文件需要配置，可以根据主题的说明进行相关的其他配置。 生成网页hexo 可以把目录生成为网页，并在本地进行测试。 命令 hexo generate，缩写为 hexo g 可以生成静态网页。 命令 hexo server，缩写为 hexo s 可以在本地生成网页服务器，默认情况下，访问网址为： http://localhost:4000/，可以观察网页的效果。 命令 hexo clean，可以用于清除生成的静态网页。 添加博文可以用 hexo new &lt;title&gt; 命令新建标题为 title 的博文，也可以在网页下的 resource/_post 目录下直接新建 .md 文档，并手动在开头添加 front-matter。 12345---title: 标题date: 创建时间tags: 标签--- 如果需要更详细的介绍，可以参考官方文档 hexo 写作 发布 github page有两种发布的方式，一种是一键部署，只把生成的静态网页内容上传到 github。另一种是 Hexo 上传部署，上传整个目录为仓库，然后进行网页生成和发布。 这里采用一键部署的形式进行发布。在目录下安装 hexo-deployer-git。 1npm install hexo-deployer-git --save 首先需要在 github 上新建一个仓库，仓库名为 &lt;username&gt;.github.io，需要选择为 public，否则需要升级 github 会员才能发布为网页。 然后，在 _config.yml 中添加 deploy 配置。 12345deploy: type: git repo: &lt;repository url&gt; #https://bitbucket.org/JohnSmith/johnsmith.bitbucket.io branch: [branch] message: [message] 其中 repo 是刚才新建的仓库 URL，branch 是要提交到仓库的哪个分支，message 是提交信息，默认会带有时间信息。 执行 hexo 命令来发布网页。先执行 hexo clean 清除之前生成的静态网页，然后执行 hexo deploy 或者缩写为 hexo d。 当执行 hexo deploy 时，Hexo 会执行 generate，并将 public 目录中的文件和目录推送至 _config.yml 中指定的远端仓库和分支中，并且完全覆盖该分支下的已有内容。 此时，在 github 仓库中已经有了 hexo 生成的静态网页内容了。去 github 仓库的 Setting 页面，选择 pages，网页的 source 选择 Deploye from a branch。保存之后等待几分钟，网页就已经发布完毕了，可以通过 Visit 进行访问。 至此，一个简单的博客已经搭建完毕了。","link":"/2024/03/23/%E5%88%A9%E7%94%A8hexo%E6%90%AD%E5%BB%BAgithub%E5%8D%9A%E5%AE%A2/"},{"title":"Kafka消息队列简单介绍","text":"消息队列是业务中常用的一种组件，它在业务中主要用于三个功能：异步，削峰，解耦。 异步是利用消息队列，把原本串行的流程改造成异步执行的流程。例如有些数据请求的处理和业务的主流程无关，或者需要延迟处理，业务就可以把相关数据丢进消息队列中，后续再从消息队列中取出消息来进行处理，不影响业务本身的主要逻辑。 削峰是指通过消息队列将可能出现的请求量突增毛刺平稳处理。例如将请求都放入一个消息队列中，业务从消息队列中读取请求进行处理。当突然有大量请求同时到来时，这些请求还是都会被放入消息队列，业务依然可以平稳地从消息队列中取出请求，依次处理，避免业务被突增的大量请求耗尽资源，导致无法提供服务。 解耦是利用消息队列进行通信，使两个不同的业务模块可以互相独立，不互相影响。例如需要记录业务操作日志的时候，主业务进行操作之后，可以将日志内容放入消息队列。日志系统不断读取消息队列的内容，根据主业务放入的内容生成日志并上传，这样主业务和日志系统就解耦了，不会因为某个业务出现问题影响到另一个业务。 基本上消息队列都是用于异步、削峰、解耦这三大功能的，在业务中遇到类似的问题的时候，都可以利用消息队列来解决。 Kafka 的基本概念比较常用的消息队列组件就是 Kafka。Kafka 是一种分布式的消息系统，用于实现高可靠性、高吞吐量、低延迟的数据传输。 基本架构一个典型的 Kafka 体系架构包括若干生产者（Producer）、若干服务代理节点（Broker）、若干消费者（Consumer），以及一个 ZooKeeper 集群。 Kafka 利用 ZooKeeper 进行集群元数据的管理和选举等操作，生产者将消息发送给 Broker，Broker 将收到的消息存储到磁盘中，消费中负责从 Broker 订阅并消费消息。 主题和分区Kafka 还有主题（Topic）和分区（Partition）两个概念。 Kafka 中的消息通过主题进行分类，每个消息在生产者发送的时候都要指定一个主题。一个主题可以分成多个分区。消息在追加到分区的时候会有一个偏移量（Offset），代表这个消息在分区的唯一标识。因此，Kafka 可以保证分区消息有序，但是不能保证主题有序。 Kafka 的分区可以存储在不同的 Broker 上，同时还引入了副本（Replica）机制，通过增加副本数量提高容灾能力。副本可以分成 leader 副本和 follower 副本，生产者和消费者只与 leader 副本交互，follower 副本只负责消息同步，相对于 leader 副本有一定的延后。 一个分区只有一个 leader，分区中的所有副本统称为 AR（Assigned Replicas），副本的同步是通过区分 ISR （In-Sync Replicas）和 OSR （Out-of-Sync Replicas）进行的。ISR 指的是和 leader 保持一定程度同步的副本，OSR 指的是和 leader 同步滞后过多的副本。理想情况是 AR=ISR，OSR 为空。 消费者只能消费 HW（High Watermark，高水位）以前的消息。一个分区集合的 HW 是由 ISR 集合中最小的 LEO（Log End Offset，代表下一条待写入消息的 offset）决定的。 发布订阅模式Kafka 的消息消费模式，常用的是发布订阅模式。 生产者是发布方，生产者将消息发送到指定 Topic，Kafka 选择合适 Partition 存储消息。 消费者是订阅方，从指定 Topic 订阅消息，按照 Partition 顺序读取，并且保证 Partition 里面的顺序不变。 消费者会组成一个消费组的逻辑概念。一个分区的消息只能被同一个消费组的消费者消费。同一个消费组内的消费者，每个消费者可以选择一至多个分区进行消费，但是一个消费组中最多只能有分区数量个消费者，多余的消费者无法分配到分区，会被闲置，无法消费消息。 Kafka 的消息特性消息队列拥有几个比较重要的特性：消息不丢失，消息有序性，消息幂等，消息积压的处理，延迟队列等。 消息不丢失消息丢失有多种原因，常见的原因如下： 生产者发送失败 异步发送消息立即返回，消息实际上没有发到 broker broker 本身问题 主从同步延迟，主分区收到消息后崩溃了 broker 持久化磁盘的时候宕机，刷盘失败 消费者消费失败 消费者消费之前提交了偏移量，但是实际上没有处理消息 消费者异步消费，提交偏移量之后异步消费失败 可以看到生产者、broker、消费者三个地方出现问题都可能导致消息丢失，那么解决方案自然也是从三个地方分别解决。 生产者方面 修改写入语义 acks，修改为 1 或者 all 0：代表发送之后就不管了，可能导致消息丢失 1：主分区写入成功后就认为发送成功，可能主分区崩溃导致消息丢失 all：写入主分区并同步给所有 ISR 成员，才认为消息发送成功 调用异步回调消息，确认发送到 broker 成功 增加异步进程扫描机制，发现漏掉的消息重新发送 broker 方面 修改刷盘参数，兼顾刷盘的效率和可靠性，一般不需要修改 利用多副本机制，ISR 和 ack 半同步来保证消息不会丢失 消费者方面 消费之后再提交偏移量 如果用到了异步消费，可以利用批量消费和批量提交的方式保证 消息有序性前面已经提到了，Kafka 只能保证分区的消息有序性，而不能保证主题的消息有序性。 很显然，最简单的保证消息有序性的方式就是一个主题只用一个分区。所有这个主题的消息都会被发送到这个分区，天然就可以保证顺序性。这个方法的缺点在于性能比较差，一个分区就代表只能有一个消费者进行消费，消费者的性能往往不能满足要求，容易导致消息积压。 第一个解决方案是单分区，消费者进行异步消费。消费者消费消息之后不进行处理，而是根据业务数据进行哈希，丢到内存队列里面，由不同的线程进行消费。由于是异步消费，这种情况下性能比较好，不过工作线程出错可能产生消息丢失或者积压的问题。 另一个解决方案是多分区，生产者在发布消息的时候根据业务数据计算出一个哈希值，然后丢到指定的分区中，这样就可以保证每个业务的消息是有序的。缺点在于可能出现数据不均匀和增加分区消息失序的问题。解决方案是通过类似 Redis 集群的 slot 的哈希形式或者一致性哈希算法，可以解决数据不均匀的问题；增加分区之后暂停一段时间再消费，可以缓解消息失序的问题。 消息幂等保证消息不会被重复消费，一方面是生产者不能重复发布同一条消息，另一方面是消费者能够识别出重复的消息进行过滤。 解决的方案比较独特，是保证消费者消费消息具有幂等的特性，也就是同样的消息只会被消费一次。在这种情况下，即使生产者发送了重复的消息，或者消费者消费了重复的消息，后续的消费过程可以保证幂等，那样就可以直接忽略重复的消息产生的影响。 保证消费者消费幂等，往往需要借助第三方组件来实现。例如在 MySQL 中插入唯一索引，重复的消息再次被消费时，因为唯一索引冲突消费失败，保证了消息只会被消费一次；或者利用 Redis、布隆过滤器等组件，判断消息已经被消费过，就直接丢弃这条消息。 消息积压的处理消息积压的核心原因还是之前提到的 Kafka 消费组的特性：一个分区只能由一个消费组中的消费者进行消费。这导致了当消息出现积压的时候，我们无法通过增加消费者的方式来提高消费速率，因此我们需要其他的手段来解决消息积压问题。 解决方案大体可以分成两类：优化分区数量和优化消费者性能 优化分区数量 在最开始的时候，最好就可以通过性能测试、压测等方式，确定吞吐量，得出能够满足性能的分区数量；或者预估发送速率、消费速率，确定一个大概的分区数量。确定了分区数量也就确定了最大的消费者数量，保证消费速率大于生产速率就可以避免积压问题。 如果是已经运行中的主题的分区数量不足，并且不允许增加分区数量的情况下，可以考虑新建一个新的主题，采用更多的分区数量。这种方法本质上还是增加分区，消费者消费老的和新的两边的消息，直到老主题的消息被消费完毕，就可以只消费新的主题，完成消费主题的切换。 优化消费者性能 消息积压一般都是消费者性能跟不上生产者生产消息的速率导致的。第一个方法是优化消费者的硬件资源，换成更好的机器；第二个方法是业务性能优化，例如业务降级、减少锁的数量、异步消费、批量消费等。注意异步消费的情况下，需要等所有异步线程执行成功再批量提交偏移量，避免消息丢失。 延迟队列延迟队列的指的是延迟消息，也就是一条消息在经过一段时间之后才会被消费。这种功能往往用于延迟推送上面，例如用户操作的相关内容需要在 10 分钟后才进行发布等等。 一些消息队列本身就支持延迟队列，例如 RabbitMQ。消息会暂时存储在一个地方，等到延迟的时间满足之后，才会被投递到真正的消息队列里面。 如果 Kafka 要实现一个延迟队列，那么往往就需要借助数据库来实现。比较通用的解决方案是利用 Redis 的 zset 来实现一个延迟队列。 业务本身做一个定时任务，例如每分钟执行一次，作为一个轮询器。生产者产生延迟消息的时候，把消息的内容作为 zset 的 member，消息延迟到期的时间戳作为 score，投递到 Redis 的 zset 结构体中。 业务的定时任务每分钟进行一次轮询，不断从 Redis 的这个 zset 中取出 score 最小的一条消息，然后判断需要投递的时间戳是否已经小于等于当前时间戳了。 小于等于当前时间戳，说明这条消息已经需要投递了，那么轮询器就把这条消息投递到消息队列中，并继续轮询取出下一条 zset 中的消息 大于当前时间戳，说明 zset 中最早的一条消息还没有到需要投递的时间，那么就重新放回 zset 中，并且结束本次轮询 需要注意的是，轮询器需要设置一个超时时间，当 zset 中消息很多的时候，可能有大量的消息同时到达了需要投递的时间，轮询器不断取出消息并投递的过程会超过一分钟，下一个轮询定时任务会启动导致两边争抢消息。因此建议轮询器设定一个超时时间，如 50s，经过 50s 后就停止从 zset 中取出数据，等待下一分钟的定时任务来继续执行。 这种方案可以适用于绝大多数的延迟队列场景，代码逻辑也比较简单。只需要注意延迟的时间不会那么精准，并且每次取出一条的性能会比较差。需要优化的话可以从批量取出、异步投递等方向进行优化。 总结总的来说，Kafka 是一个比较常用的消息队列组件，了解一些 Kafka 和消息队列的基础知识，对于业务的性能优化、架构设计都有很大的帮助。 消息队列的每一个的特性都可以拓展成复杂的文章，想要继续深入了解 Kafka 的相关知识，大量的资料和实践都是必不可少的。 参考资料 《深入理解 Kafka：核心设计与实践原理》，朱忠华，电子工业出版社 极客时间《后端工程师的高阶面经》","link":"/2024/03/30/Kafka%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/"},{"title":"Elastic Search集群存储和索引搜索原理简介","text":"本文将从 ElasticSearch 的集群到索引内部，简单介绍一下 ES 的存储和搜索的实现原理，不会涉及具体的搜索语句实现和搜索技巧。本文主要分为四个部分，首先简要介绍 ES 的基本概念，然后从大到小，介绍 ES 集群的概念和分片在集群中的存储方式，每一个索引在内存和磁盘的存储过程，最后简单介绍一下 ES 如何通过索引快速检索对应的文档。 部分图片和内容来自 ElasticSearch 官方文档和相关博客，参考资料已经列在文章最下方。 ES 简单介绍Elasticsearch（简称 ES）是一个分布式、可扩展、开源的全文搜索与数据分析引擎。ES 支持近乎实时的大量数据存储、搜索和分析。ES 不仅仅只是全文搜索，还支持结构化搜索、数据分析、复杂的语言处理、地理位置和对象间关联关系等。 ES 的特点： 一个分布式的实时文档存储引擎，每个字段都可以被索引与搜索 一个分布式实时分析搜索引擎，支持各种查询和聚合操作 能胜任上百个服务节点的扩展，并可以支持 PB 级别的结构化或者非结构化数据 常用场景 搜索引擎例如电商商店的商品搜索，站内搜索，模糊查询和全文检索等服务。 非关系型数据库例如应用 ES 来实现业务宽表，优化原本冗余的数据库查询。 大数据近实时分析引擎存储大数据，并做到近实时的分析。 日志系统例如 ELK，通过收集日志文件存储到 ES，实现快速高效的日志检索。 搜索过程ES 的搜索过程主要分为四大步骤。 查询分析用自然语言处理输入的文本，进行纠错和标准化，例如去除大小写差异，修改复数形式； 分词技术对用户输入的查询文本分词，分成多个关键词； 关键词检索提交关键词之后在索引中进行检索； 搜索排序根据相关度排序检索结果输出； ES 集群原理集群基本概念 近实时文档被索引和文档可被检索之间仅有短暂延迟（约 1 s） 集群一或多个共同存储数据、提供索引和搜索能力的节点的集合 节点集群的每个服务器，参与存储索引和提供搜索能力 索引一系列拥有相似特征的文档的集合 文档可被索引的基础信息单元 分片和复制索引可以分片存储，也可以指定分片备份的数量 集群原理索引是保存相关数据的地方，实际上是指向一个或者多个物理分片的逻辑命名空间。分片是一个底层的工作单元，一个分片是一个 Lucene 的实例，它本身就是一个完整的 Lucene 索引。 一个分片可以是主分片或者副本分片，在索引建立时就已经确定了主分片数，但是副本分片数可以随时修改。索引内任意一个文档都归属于一个主分片，所以主分片的数目决定着索引能够保存的最大数据量。一个副本分片只是一个主分片的拷贝，副本分片作为硬件故障时保护数据不丢失的冗余备份，并为搜索和返回文档等读操作提供服务。 图中展示了一个包含 3 个节点的集群，有三个主分片 p0, p1, p2，每个主分片都包含两个副本分片。在增加新的分片的时候，ES 会分散节点的负载，重新分配分片所在的节点。分片的分配原则是副本分片和主分片一定不在一个节点中，如果只有一个节点，那就不会保存副本分片，此时集群的健康状态也会标记成 yellow。 分片操作 添加分片 例如我们添加三个主分片，每个主分片有一个备份分片到只包含一个节点的集群。 由于此时集群中只有一个节点，因此三个主分片都会被添加到这个节点中，而备份分片不能和主分片放在同一个节点，因此没有地方放置备份分片，当前集群仅包含一个包含 3 个主分片的节点，集群的健康状态为 yellow。 扩展集群 我们向集群中添加一个节点，此时可以添加备份了，因此新的节点中将包含三个主分片各自的备份分片。 扩容节点 如果我们继续向集群中添加节点进行扩容，那么 ES 还会重新排列分片所在的节点，分散节点的负载，当然还是会保证主节点和备份节点不在同一个节点上。 调整备份分片数 我们设置每个主节点拥有两个备份分片，此时每个主节点都有两个备份分片，填充到其他的节点中。 出现故障 例如节点 1 被关闭，ES 会重新选举一个主节点，并将已经不存在的主分片的备份分片升级为主分片，保证集群能够正常工作。但是上面已经设定每个主分片要有两个备份分片了，而此时每个主分片仅剩下一个备份分片，因此集群的健康状态变为 yellow。 文档存储决定文档会存储到哪个分片是由文档路由公式计算得出的。文档路由公式可以简单理解为一个哈希函数，根据文档的 id 计算文档应该存储到哪个分片中。 $shard = hash(routing) % number_of_primary_shards$ 公式中 routing 一般指的是文档 id，经过哈希函数计算出一个数字，然后取模分片总数来决定存储到哪个分片。注意上面已经说过，主分片的数目在创建索引的时候就已经确定了，所以不会产生新增主分片导致的 rehash 问题。 新建文档 如图所示，其中 node1 节点被标记成了 master 节点。 在 ES 中，master 节点负责管理集群的状态和调度，以及保存和更新集群的一些元数据信息，之后会同步到所有节点，所以集群中每个节点都需要保存全量的元数据信息。master 节点仅仅负责集群和索引状态的管理，查询和存储都不用经过这个 master 节点。除此之外，还有负责调度的 client 节点，以及存储数据的 data 节点这两种集群节点角色，在此处不做明显区分。 元数据信息主要包含以下几种： 集群的配置信息 集群的节点信息 模板 template 设置 索引以及对应的设置，mapping、分词器和别名 索引关联到的分片以及分配到的新节点 新建文档的步骤可以大致分为三步。 客户端向 node1 发送新建、索引或者删除请求； 节点通过文档 _id 和路由公式计算出文档属于哪一个分片，假设属于 p0，那么请求会被转发到 node3，因为 p0 的主分片目前分配在 node3 上； node3 在主分片上面执行请求，如果成功了，它将请求并行转发到 node1 和 node2 的备份分片上，一旦所有的备份分片都报告成功，那么 node3 将向协调节点报告成功，协调节点向客户端报告成功； 在客户端收到成功响应的时候，文档变更已经在主分片和所有副本分片执行完成了，因此该变更是安全的。 文档检索 文档的查询阶段包含三个步骤： 客户端发送一个查询请求到 node3，node3 会创建一个大小为 from+size 的空优先队列（此处 from+size 来自查询请求）； node3 将查询请求转发到索引的每个主分片或备份分片中，每个分片在本地执行查询并添加结果到 from+size 的本地有序优先队列； 每个分片返回各自优先队列的所有文档 id 和排序值（ES 计算的查询的 score）给协调节点，协调节点合并这些值到自己的优先队列，产生全局排序后的结果； 上述先查后取的过程支持用 from 和 size 参数分页，但是显然，如果参数过大会有 深分页 的问题。传递信息给协调节点的每个分片都必须先创建一个长度为 from+size 的 ui 咧，协调节点会得到 numbser_of_shards*(from+size) 长度的队列，并要根据这个队列来找到 size 限制的文档。 es 中会通过 scroll 来缓存查询的结果，在一定程度上减少深分页的影响。 文档取回 文档检索的过程仅仅是获得了要返回的文档的 id，然后还需要执行文档取回的过程将文档返回给客户端。 分布式取回阶段由以下四个步骤组成： 协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 GET 请求。 每个分片加载文档，如果有需要的话，接着返回文档给协调节点。 一旦所有的文档都被取回了，协调节点返回结果给客户端。 协调节点首先决定哪些文档“确实”需要被取回。例如，如果我们的查询指定了 { “from”: 90, “size”: 10 } ，最初的 90 个结果会被丢弃，只有从第 91 个开始的 10 个结果需要被取回。这些文档可能来自和最初搜索请求有关的一个、多个甚至全部分片。 ES 索引存储倒排索引分片和索引的关系：ES 的索引是分片的集合；分片是一个 Lucene 倒排索引， 倒排索引 和倒排索引相对的就是正排索引，也就是传统数据库中，存储文档和文档内容的关系。每个文档有一个文档 id 作为一行，后面列出了文档包含的所有单词。这种存储方式无法应对全文检索的场景。文本字段中的每个单词需要被搜索，例如要查询包含“人工”的文档，对数据库意味着需要单个字段有索引多值的能力。Elasticsearch 是通过 Lucene 的倒排索引技术实现比关系型数据库更快的过滤。特别是它对多条件的过滤支持非常好。倒排索引就是对于每一个词项，存储它出现过的文档 id。例如某个文档出现了“人工智能”这一条。其中“人工”这个词项出现在了文档 1，2，3 中，所以倒排索引中就会记录 人工| 1，2，3。 不变性 在 ES 中，倒排索引具有不变性，也就是倒排索引被写入后是不可改变的。 选择不更改倒排索引是因为不变性具有很多优点： 不需要锁。如果你从来不更新索引，你就不需要担心多进程同时修改数据的问题。 一旦索引被读入内核的文件系统缓存，便会留在哪里，由于其不变性。只要文件系统缓存中还有足够的空间，那么大部分读请求会直接请求内存，而不会命中磁盘。这提供了很大的性能提升。 其它缓存 (像 filter 缓存)，在索引的生命周期内始终有效。它们不需要在每次数据改变时被重建，因为数据不会变化。 写入单个大的倒排索引允许数据被压缩，减少磁盘 I/O 和需要被缓存到内存的索引的使用量。 自然，不变性也导致了一些缺点，最大的缺点就是无法修改。当我们需要向索引中新增文档时，只能重建整个索引，这就限制了索引包含的文档数量，或者是索引可被更新的频率。 索引更新如何在保证倒排索引不变性的情况下更新索引？ES 选择建立更多的索引。 ES 提出了按段搜索的概念，每一个段都是一个倒排索引，提交点（commit point）列出了目前所有已知的段。在搜索的时候，所有已知的段都会被检索，最后会进行合并。 新增文档时，新文档被收集到内存索引缓存，随后被提交。 一个新的段（即追加的倒排索引）被写入磁盘。 一个新的包含段名字的提交点被写入磁盘。 磁盘进行同步 — 所有在文件系统缓存中等待的写入都刷新到磁盘，以确保它们被写入物理文件。 新的段被开启，让它包含的文档可见以被搜索。 内存缓存被清空，等待接收新的文档。 删除更新时，并不会真正删除，而是在提交点的 .del 文件中被标记为删除。后续在段合并的时候才会被真正删除。 近实时搜索按照上述所说的更新索引的方式，一个新的文档需要在写入磁盘后才可以被搜索到，这导致 fsync 成为了新增文档和可被检索之间的性能瓶颈。为了降低新增文档和可被检索之间的延迟，实现近实时的搜索，ES 将新的段写入文件系统缓存来使新的段可被搜索（每秒一次）。之所以我们说 ES 的搜索是近实时搜索，有 1 s 左右的延迟，就是因为 ES 每秒一次将新的段写入缓存。 如果所示，Lucene 允许新段在写入提交点之前就可以打开，使其包含的文档在未进行一次完整提交时便对搜索可见。这种方式比进行一次提交代价要小得多，并且在不影响性能的前提下可以被频繁地执行。 持久化变更为了实现近实时搜索，ES 每秒将段写入缓存而不是写入磁盘，那么就可能产生数据丢失的问题。为保证在断电等情况发生的时候也不会丢失数据，ES 增加了持久化日志（translog）。 一个文档被索引之后，就会被添加到内存缓冲区，并且追加到 translog； 分片每秒被 refresh 一次（写入缓存，可被搜索，没有写入磁盘）。此时，这些在内存缓冲区的文档被写入到一个新的段中，且没有进行 fsync 操作。这个段被打开，使其可被搜索。内存缓冲区被清空。 每隔一段时间———例如 translog 变得越来越大，索引被刷新（flush）, flush 操作往往是异步的。；所有在内存缓冲区的文档都被写入一个新的段。缓冲区被清空。一个提交点被写入硬盘。文件系统缓存通过 fsync 被刷新（flush）。老的 translog 被删除。 一个新的 translog 被创建，并且一个全量提交被执行。 段合并由于自动刷新流程每秒会创建一个新的段，这样会导致短时间内的段数量暴增。 段数目太多会带来较大的麻烦：每一个段都会消耗文件句柄、内存和 cpu 运行周期；更重要的是，每个搜索请求都必须轮流检查每个段，所以段越多，搜索也就越慢。 Elasticsearch 通过在后台进行段合并来解决这个问题。小的段被合并到大的段，然后这些大的段再被合并到更大的段。段合并的过程在搜索时会自动执行。老的段会被删除。同时，之前所说的需要被删除的文档，即带有 .del 标记的文档会被丢弃。 写入流程总结综合以上种种写入方式和技巧，总结一下 ElasticSearch 的写入过程如下： 数据写入内存 buffer 和 translog 文件； Buffer 满或过一段时间（1 s），ES 将内存 refresh 到段文件，存储到系统内存中，并清空 buffer。此时文档可被检索； 重复 1-2 步，直到 translog 文件较大或一段时间后，执行 commit；Commit 分成几步： Refresh，清空 buffer； 写入 commit point，记录已提交的段文件； Fsync，持久化存储到磁盘； 删除旧的 translog； Merge： 磁盘中读出小的段，与内存中的段进行合并； 生成新的段，commit 时写入磁盘； 创建新的 commit point，去掉旧的段； 删除磁盘中旧的段文件； ES 搜索原理索引组成ES 的索引可以分成三个部分。 Posting List所有符合 term（查询条件）的文档 id； Term DictionaryTerm 按顺序排列的组合，分 block 保存，每个 block 按前缀压缩; Term IndexTerm 的前缀部分，是一颗前缀树，并通过 FST 进行压缩，全部存在内存中; 经过压缩之后能够全部存储在内存中，检索文档的效率就很高了。之所以说 ES 的检索效率比 MySQL 快，就是因为 Mysql 只有 term dictionary 这一层，是以 B+树排序的方式存储在磁盘上的。在 MySQL 中检索一个 term 需要若干次的随机访问操作。而 Lucene 在 term dictionary 的基础上添加了 term index 来加速检索。term index 以 Trie 树的形式缓存在内存中，并通过有限状态转换器进行压缩，可以在 $O(1)$ 查找。 压缩技巧 Frame of Reference 编码 对于频繁出现的词条（如性别）可以极大减少磁盘占用，主要分成三步： Posting list 差分编码； 分块； 按位压缩； Roaring Bitmap 对每个文档 id 计算两个值：(id/65536, id%65536)； 根据第一个值进行分块； 每个块根据是否大于 4096，编码成 bitmap 或者单纯的数组； 联合搜索联合搜索就是将多个 term 查询得到的 posting list 进行合并。直接遍历进行合并的效率比较低，为了提高合并的效率，ES 采用了跳表的方式来加速合并。 跳表 跳表（skip list）的设计初衷是作为替换平衡树的一种选择。因此可以理解成与红黑树类似，一种可以快速实现查询、插入、更新、删除的数据结构。在并发环境下更新操作更加高效，同时原理比红黑树简单。 跳表就是在普通单向链表的基础上增加了一些索引，而且这些索引是分层的，从而可以快速地查的到数据。 合并过程 首先，遍历第一个 posting list，第一个节点为 2；去第二个 posting list 中查找 2，没有就找更大的，找到 13；返回第一个 posting list，查找 13，找到了；查找第二个 posting list，已经是 13；查找第三个 posting list，找到了 13，所有都找到了，记录；遍历第一个列表的下一个，是 17；第二个列表查询，没有，找到更大的 22；返回第一个列表，查询 22，没有，找到更大的 98；去第二个列表查询 98…… 依次类推，最后得到查询的结果 [13, 98]。 在遍历的过程中跳过了一部分节点，因此速度很快，分块也可以用上 Frame of Reference 编码进行压缩。 参考文档ES官方文档ES权威指南理解Elasticsearch工作原理时间序列数据库的秘密——索引跳表原理skip lists: a probabilistic alternative to balanced trees编码方式的比较","link":"/2024/03/24/ES%E9%9B%86%E7%BE%A4%E5%AD%98%E5%82%A8%E5%92%8C%E7%B4%A2%E5%BC%95%E6%90%9C%E7%B4%A2%E5%8E%9F%E7%90%86%E7%AE%80%E4%BB%8B/"},{"title":"Raft算法基础介绍","text":"Raft 算法是一种管理复制日志的一致性算法，主要分成四个部分，领导选取，日志复制，安全和成员变化。 简单来说，Raft 就是在集群中选举一个领导者，客户端写操作直接写给领导者，领导者更新本地日志，并将日志复制到其他节点。大多数节点返回处理成功之后，领导就返回客户端更新成功，并下发更新命令，让大多数节点更新数据。 Raft 算法被广泛应用于集群同步之中，例如 Redis 哨兵集群的选举等。本文是对 Raft 算法基本内容的简单介绍，不涉及具体的算法实现和细节。 如果想要更直观、详细地了解 Raft 算法，推荐通过 Raft 动画网站来更直观地了解 Raft 算法的基本流程，以及阅读 Raft 论文译文来获取更加详细的信息。 领导选取Raft 的节点有三种状态：跟随者，候选人和领导者。 一开始，每个节点都是跟随者，经过领导选举的流程，会产生一个领导者。在正常情况下，一个集群只有一个领导者，负责接收客户端消息以及日志同步，剩下的都是跟随者。 当跟随者有一个选举时间没有收到来自领导者的消息后，就会把自己变成候选人，并给自己投票。这里还会额外记录这是第几次选举，也就是任期号。 节点变成候选人之后，会给其他的机器发送竞选通知。在此期间，候选人节点会一直保持候选人状态，直到以下三种情况发生： 候选人赢得选举成为领导者 其他节点收到竞选通知之后，按照先来先服务的原则，如果自己还没有变成候选人且没有给其他的候选人节点投票过，就会投票给该候选人节点。如果该候选人节点得到了大多数的选票，就会变成领导者。 其他候选人赢得选举成为领导者 其他节点收到竞选通知之后，已经给其他的候选人节点投票了，就会忽视该候选人节点的通知。如果其他候选人节点得到了大多数的选票，那个候选人节点就会成为领导者，开始发送领导者同步消息。 该候选人节点收到领导者同步消息，并且判断是当前或者更晚的任期选举出来的领导者，就知道已经有了其他的领导者，自动退出候选人状态成为跟随者。 一段时间之后没有选出领导者 这种情况一般出现在好几个候选人的情况下，导致所有候选人都没有得到大部分选票。在这种情况下，每一个候选人都会选举超时，之后会依据随机的选举超时时间，再次重复领导选举过程。由于随机的选举超时时间，下一轮选举中候选人出现的时间会有差别，这让这种情况很少发生。 日志复制领导者会按照一个时间定时给其他的机器发送心跳请求，其他机器收到后也会回复。这个时间需要远小于选举超时时间，避免在领导者正常工作的情况下，因为网络延迟等原因让跟随者收不到领导者的消息，错误认为当前集群没有领导者。 当客户端请求的时候，领导者先更新自己本地的日志，然后把日志通过心跳请求复制给其他的机器，其他机器更新本地日志后，会在回复心跳的时候通知领导者。这里如果有机器没有回包，领导人即使向客户端回包了，也会无限重试，直到所有的跟随者都存储了日志。 领导者收到大多数机器的更新之后，就会执行日志更新本地数据，通知客户端请求成功。下一次心跳，领导者会把 commit 请求发给其他机器。其他机器收到后，也会更新自己的本地数据。 安全性选举限制选举的时候会保证领导者一定有所有的日志。投票请求会包含当前机器最新的日志，如果跟随者发现候选人发来的投票请求中，日志没有自己新，那就会拒绝投票。 基于这个原理，可以证明领导者一定拥有之前提交的日志信息。简单来说就是大多数机器都复制了才会提交，而提交之后大多数机器都有最新的日志，不会投票给没有日志的候选人。 另外，服务器有当前领导者存在的时候，不会响应要求投票的请求，这保证了旧配置服务器被移除的时候它可能会进行选举过程的问题。 避免脑裂如果网络之间出现问题，例如 5 台机器，2 台机器和其他 3 台机器不通。 这种情况下，这个集群会出现两个领导者。其中一个是原本的领导者，位于 2 台机器的集群；另一个是 3 台机器的集群新选举出来的。但是 2 台机器的小集群因为得不到大多数机器的更新确认信息，所以无法更新数据，而 3 台机器的集群则可以更新。 当网络恢复后，2 台机器的集群收到 3 台机器集群的心跳请求，因为 3 台机器的集群的任期号更高，所以会采用 3 台机器集群的日志和数据，保证了集群整体还是一致的。 参考资料 Raft 动画 Raft 论文译文","link":"/2024/04/03/Raft%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80%E4%BB%8B%E7%BB%8D/"},{"title":"缓存模式和缓存问题介绍","text":"缓存是用来临时存储数据的，一般来说使用缓存的目标是用来提升性能，缓解数据库的压力。 有三种常见的缓存读写策略： Cache Aside Read/Write Through Write Behind 三种缓存模式的读写特点Cache Aside（旁路缓存模式）Cache Aside 模式的特点是将数据库视为数据存储的主要位置，也就是数据库存储的数据一定是最准确的，缓存仅仅是用来缓解数据库压力，提高读取性能而被使用。 读请求： 请求查询缓存，缓存中有数据，就直接返回给客户端 缓存中没有数据，就会去读数据库 取到数据之后，将数据放入缓存，然后返回给客户端 写请求： 数据写入数据库 删除缓存中对应的数据 Read/Write Through（读写穿透模式）Read/Write Through 模式的特点是把缓存视为数据存储的主要位置，缓存基本充当了数据库的功能，而用缓存层的服务来同步数据到数据库作为备份使用。 这里 Read/Write Through 和 Cache Aside 的主要区别在于在缓存层有一层封装，不需要业务对缓存、数据库进行操作，而是由这层抽象封装层来完成。 读请求： 请求查询抽象封装层，抽象层查询缓存，缓存中有数据，就直接返回给客户端 缓存中没有数据，抽象层从数据库加载，写入缓存后返回 写请求： 缓存中有数据，直接更新缓存中的数据，利用抽象封装层去更新数据库 缓存中没有数据，直接更新数据库 Write Behind（异步缓存写入）Write Behind 模式的特点是只更新缓存，不直接去操作数据库。而是利用消息队列等异步的方式去更新数据库的数据。 这种模式实际上是直接将缓存当做数据库来使用，数据库只是用来作为备份的，适用于数据经常变化，对于一致性要求不高的场景。 读请求： 直接查询缓存，缓存中存在直接返回给客户端 缓存中不存在，查询数据库返回给客户端 写请求： 直接更新缓存 异步任务，例如消息队列等更新数据库 缓存不一致缓存不一致的最主要原因，就是在并发访问的模式下，各种操作之间的延迟和先后顺序不一致，导致可能出现数据库和缓存更新时机产生差别情况。 而在 Cache Aside 模式中，已经通过设计尽可能规避了缓存不一致的情况：分别是写入时删除缓存而非更新缓存，以及先写入数据库再删除缓存。 删除缓存而不是更新缓存 如果同时来了写请求更新了数据库，但是后来的请求因为网络原因先更新了缓存，就会导致先来的请求后更新缓存，把缓存更新成非最新值。 通过删除缓存的方式来做，就可以保证不会出现这种问题。因为只会删除缓存，不会修改缓存的值。 此外，写场景频繁，或者是写入值需要计算才能得出的场景，更新缓存会浪费性能。 写入的时候先写入数据库而不是缓存 如果先写入缓存，在写入缓存之前删除缓存的时候，有新的读请求就会因为缓存 miss 去读取数据库，然后更新缓存，导致缓存中存在老的数据。 先写入数据库，除非删除缓存操作失败，不然不会导致缓存数据不一致。 延迟双删方案先写缓存也可以保证数据一致性，方法是延时删除缓存。 先删除缓存 写入数据库 休眠一会，再次删除缓存 这种方案可以缓解数据不一致的问题，只有在休眠的时候可能产生脏数据，例如休眠期间其他并发线程写入缓存数据，导致第二次删除之前数据不一致，可能性比较小。更大的问题还是频繁删除缓存，会更容易出现缓存未命中的问题。 缓存问题说到缓存，另一个比较常见的问题关于三种缓存问题的概念。这里也简单列出这三种缓存模式的定义和解决方案。 最通用的解决方案就是对请求进行限流。缓存问题的本质是数据库无法承受大批量的数据请求，如果能够限流请求的数量，到光靠数据库也可以扛住的程度，那么这些缓存问题就全部迎刃而解。 缓存穿透数据既不在缓存之中，也不在数据库之中。 表现： 请求缓存没数据，压力到数据库 数据库也没数据，没法回写缓存，下次请求还是没有，数据库依然有压力 解决方法： 查询数据库不存在时回写特殊值 查询缓存得到特殊值，知道数据不存在，就不去查数据库了 布隆过滤器限流 先通过布隆过滤器判断数据是否存在 可能存在的情况下再去查询缓存和数据库 缓存击穿数据不在缓存中，而且是热点数据。注意，如果是非热点数据，就只是普通的缓存未命中，并不算是缓存击穿。 表现： 热点数据查询，查询缓存时没数据，需要查询数据库 数据库收到大量查询请求，压力突增，性能下降 解决方法： singleflight 相同的查询仅有一个查询会真正查询缓存 其他的查询等待这一个查询的结果，而不是都去查询缓存 热点数据不过期 业务提前判断可能出现缓存击穿的热点数据 对于这些数据在缓存中延长过期时间，或是永久存储 缓存雪崩大量数据同一时间过期。 表现： 大量数据过期，大量请求查询缓存时未命中，需要查询数据库 数据库收到大量查询请求，压力突增，性能下降 解决方法： 随机过期时间 大批量数据插入缓存的时候，不要设置统一的过期时间 随机设置过期时间，数据会在一段时间内平稳过期，减少雪崩的概率 参考资料 美团二面：Redis与MySQL双写一致性如何保证？ Redis 有几种缓存读写策略 缓存问题：怎么解决缓存穿透、击穿和雪崩问题？","link":"/2024/04/06/%E7%BC%93%E5%AD%98%E6%A8%A1%E5%BC%8F%E5%92%8C%E7%BC%93%E5%AD%98%E9%97%AE%E9%A2%98%E4%BB%8B%E7%BB%8D/"},{"title":"最短路算法小结","text":"最短路算法，简单来说就是求出给定的图中，两点或者多点之间距离最短的路径的算法。一般来说，这种算法不会特意拿出来出题，但是如果拿出来出题了，而我们没有掌握相关算法的话就会一筹莫展。 本文将简单介绍几种常用的最短路算法，和它们的 C++实现与例题。 Dijkstra 算法Dijkstra 算法是用于求解非负权图上单源最短路径的算法。核心要点是图上不能有权重为负数的边，否则就不适用 Dijkstra 算法。 Dijkstra 算法的过程很简单： 将所有节点划分为两个集合，已确定最短路的点集 S，和未确定最短路的点集 T 从 T 集合中选择一个最短路长度最小的节点，移动到 S 集合 对刚才选择的节点的所有出边进行松弛（relax）操作 重复 2-3 步，直到 T 集合为空 需要提一下的是松弛操作，对边 (u, v) 的松弛操作，指的是用节点 u 的最短路+边 (u, v) 的边权重，和节点 v 的最短路进行比较，看看是否需要更新最短路，公式如下： 1dis(v)=min(dis(v), dis(u)+w(u,v)) 简单的 Dijkstra 的 C++参考代码如下： 123456789101112131415161718192021const int mxn=1005;int g[mxn][mxn],dis[mxn],vis[mxn]; void dijkstra(int s) //s为起点 { //如果需要路线，就再开一个数组记录 memset(dis,0x3f,sizeof(dis)); memset(vis,0,sizeof(vis)); vis[s]=1;dis[s]=0; for (int i=0;i&lt;n;i++) //n是点数(mxn)，这里需要循环次数为n次 { for (int j=0;j&lt;n;j++) //这里需要注意点的编号 { if (g[s][j]!=0) dis[j]=min(dis[j],dis[s]+g[s][j]); } for (int j=0;j&lt;n;j++) { if (!vis[j]&amp;&amp;(vis[s]||dis[s]&gt;dis[j])) s=j; //找那一行不属于集合已经找到的且最小的作为下一个起点 } vis[s]=1; }} //这样就求出了所有点到s的最短路 堆优化Dijkstra 算法的优化，主要是针对第 2 步，从集合 T 中选择一个最短路长度最小的节点。 常规的暴力算法，整体的算法时间复杂度是 O(n^2)，而比较常用的优化方式就是利用堆来进行优化。每次成功松弛一条边，就将这条边放入堆中。下次从集合 T 取出最短路长度最小的节点时，就可以直接取出堆顶端的节点来代替。 优化后的算法时间复杂度为 O(mlogn)。注意，如果边大于点的话，堆优化后可能会更慢。 1234567891011121314151617181920212223242526const int mxn=1005;int g[mxn][mxn],dis[mxn],vis[mxn];priority_queue&lt;pair&lt;int, int&gt;, vector&lt;pair&lt;int, int&gt; &gt;, greater&lt;pair&lt;int, int&gt; &gt; &gt; q; void dijkstra(int s) //s为起点{ //如果需要路线，就再开一个数组记录 memset(dis,0x3f,sizeof(dis)); dis[s]=0; q.emplace(dis[s], s); while(!q.empty()) { auto [d, s] = q.top(); q.pop(); if (d&gt;dis[s]) continue; for (int j=0;j&lt;n;j++) //这里需要注意点的编号 { if (g[s][j]!=0) { if (dis[j]&lt;dis[s]+g[s][j]) { dis[j]=dis[s]+g[s][j]; q.emplace(dis[j],j); } } } }} //这样就求出了所有点到s的最短路 Bellman-Ford 算法Bellman–Ford 算法是一种基于松弛操作的最短路算法，可以求出有负权的图的最短路，并可以对最短路不存在的情况进行判断。 每次循环的时候，算法都会尝试对所有的边进行一次松弛。在最短路存在的情况下，每次松弛都会确定一条最短路的边，所以最多只需要 n-1 次松弛就可以得到最短路，时间复杂度为 O(nm)。 从 Bellman–Ford 的实现方式也可以看出，如果图中存在负环，那么松弛操作就会一直进行下去。因此 Bellman–Ford 算法也可以用来判断图中是否存在负环：如果在进行了 n-1 轮松弛操作之后，还存在可以松弛的边，就说明图中存在负环。 12345678910111213141516// 经典算法void bf() { // 起始先将所有的点标记为「距离为正无穷」 Arrays.fill(dist, INF); // 只有起点最短距离为 0 dist[k] = 0; // 迭代 n 次 for (int p = 1; p &lt;= n; p++) { int[] prev = dist.clone(); // 每次都使用上一次迭代的结果，执行松弛操作 for (Edge e : es) { int a = e.a, b = e.b, c = e.c; dist[b] = Math.min(dist[b], prev[a] + c); } }} SPFA 算法优化SPFA，Shortest Path Faster Algorithm，是对 Bellman-Ford 算法的一种优化。 在 Bellman–Ford 算法中，只有上一次被松弛的边连接的节点，才可能引起下一次的松弛操作。我们可以用队列来维护可能会引起松弛操作的节点，就可以只访问必要的边。 SPFA 也可以用来判断是否存在负环，当最短路经过了 n 条边时，就可以知道已经到达了一个负环，因为一般的最短路的长度最多是 n-1。 1234567891011121314151617181920212223242526272829303132const int mxn=1005;int q[mxn],head,tail,cnt; //循环队列int first[mxn],nxt[mxn&lt;&lt;1],vv[mxn&lt;&lt;1],cost[mxn&lt;&lt;1]; //数组模拟邻接表int dis[mxn],iin[mxn]; //判断是否在队列内 void spfa(int s,int t) //s起点，t终点（其实不传也没什么……） { memset(dis,0x3f,sizeof(dis)); memset(iin,0,sizeof(iin)); dis[s]=0; //这步不要忘了……虽然即使忘了测试时也会检查出来…… q[tail]=s;iin[s]=1;cnt++;tail=(tail+1)%mxn; //入队 while (cnt!=0) { int u=q[head]; iin[u]=0;cnt--;head=(head+1)%mxn; //出队 for (int i=first[u];i!=-1;i=nxt[i]) { int v=vv[i]; if (dis[v]&gt;dis[u]+cost[i]) //可以松弛 { dis[v]=dis[u]+cost[i]; if (iin[v]==0) { iin[v]=1; q[tail]=v; tail=(tail+1)%mxn; cnt++; } } } }} //dis[t]即为到t的最短路（具体看点的编号） Floyd 算法用来求任意两个节点之间最短路的多源最短路算法，时间复杂度比较高为 O(n^3)。在需要求多源最短路的情况下可以使用。 12345678910111213const int mxn=1005;int g[mxn][mxn];void Floyd(int n) //n为点的个数 { memset(g,0x3f,sizeof(g)); for (int i=0;i&lt;n;i++) g[i][i]=0; for (int k=0;k&lt;n;k++) //注意k要在最外面 for (int i=0;i&lt;n;i++) for (int j=0;j&lt;n;j++) if (g[i][j]&gt;g[i][k]+g[k][j]) g[i][j]=g[i][k]+g[k][j];} //g[i][j]即为i到j的最短路 小结DIjkstra 算法是最常用的求最短路的算法，可以求出某一个点到其他点的最短路。需要注意 Dijkstra 算法的两个适用范围： Dijkstra 求出的是从某一个点出发的单源最短路径 Dijkstra 算法要求图中不存在权值为负数的边 如果图中存在权值为负数的边，那就需要用 Bellman-Ford 算法以及它的优化 SPFA 算法；如果需要求所有点的多源最短路径，那就需要用 Floyd 算法进行求解。 参考资料 最短路-OI Wiki","link":"/2024/05/05/%E6%9C%80%E7%9F%AD%E8%B7%AF%E7%AE%97%E6%B3%95%E5%B0%8F%E7%BB%93/"},{"title":"groupcache简单介绍","text":"简单介绍基本介绍groupcache 是一个 Go 语言的分布式的缓存库，主要特点是支持分布式集群的部署方式。传统的 Memcached 的数据是缓存在单个实例中的，每个实例都有自己的一套缓存，而 groupcache 中，同一个集群中，访问任何一个实例都可以得到数据。 groupcache 最显著的特征是只用于存储静态资源，资源在缓存中不支持修改。groupcache 缓存库中不提供 update 和 delete 等方法，只有 get 一种方法。 groupcache 在获取缓存数据的时候，首先从本机缓存和本次存储的热点缓存中获取。如果获取失败，会通过 http 调用去查询 peer 机器上的缓存，如果还是没有，才去底层数据存储中获取。在获取数据的过程中，groupcache 会通过 singleflight 来保证相同的请求只访问一次数据库。 对比 Memcached文档中介绍了的 groupcache 相比 memcached 的区别： 不需要运行单独的服务，以库调用的方式给客户端使用 自带缓存填充机制，缓存未命中时只有一个进程会去加载并填充数据 不支持版本化数据，存储的数据不可更改，没有过期时间和淘汰机制等 这也意味着 groupcache 支持热点数据的镜像存储，避免了热点负载问题 目前仅能用于 Go 语言 基本流程名词解释 group 缓存的主要结构体，在最开始创建缓存的时候会指定 group 和名称。 peer 存储数据的节点，是通过一致性哈希来决定 key 对应的数据存储在哪个 peer 节点上的。 mainCache peer 节点本地的缓存。 hotCache 节点本地的热点数据缓存，在查询数据后有十分之一的概率把数据作为热点数据存在本地。这样做的好处是真正的热点数据会有高概率存储在各个 peer 节点上。 请求流程 用户发起请求到 httppool 的监听节点，该监听节点本身也相当于一个 peer 节点 分析请求，根据 groupname 调用相应的 group 查询本地 mainCache 和 hotCache，找到数据就返回 没有找到数据，查询当前 peer 节点是否是当前请求对应的 peer 节点 是的话就从本地数据集获取数据，存放在 mainCache 缓存中 否则就从远程对应的 peer 节点获取数据，获取后有 1/10 的概率存储在 hotCache 中 返回数据 组件分析groupGroup 是缓存的主要结构体，包含了获取缓存的完整流程，它的结构体字段如下所示。 123456789101112131415161718192021222324252627282930313233type Group struct { name string getter Getter peersOnce sync.Once peers PeerPicker cacheBytes int64 // limit for sum of mainCache and hotCache size // mainCache is a cache of the keys for which this process // (amongst its peers) is authoritative. That is, this cache // contains keys which consistent hash on to this process's // peer number. mainCache cache // hotCache contains keys/values for which this peer is not // authoritative (otherwise they would be in mainCache), but // are popular enough to warrant mirroring in this process to // avoid going over the network to fetch from a peer. Having // a hotCache avoids network hotspotting, where a peer's // network card could become the bottleneck on a popular key. // This cache is used sparingly to maximize the total number // of key/value pairs that can be stored globally. hotCache cache // loadGroup ensures that each key is only fetched once // (either locally or remotely), regardless of the number of // concurrent callers. loadGroup flightGroup _ int32 // force Stats to be 8-byte aligned on 32-bit platforms // Stats are statistics on the group. Stats Stats} 可以看到，Group 中包含了本地缓存的完整内容，核心是 Get 方法。 首先查询本地的 mainCache 和 hotCache，获取 key 对应的数据值，查询到就直接返回 如果不存在，就通过 PickPeer 方法，查询 key 对应的节点，发起 http 请求从对应的节点获取数据 如果 key 对应的节点就是本地，那就调用用户定义的 Getter 方法，获取原始数据 consistenthashgroupcache 通过一致性哈希的方式来分配 key 存储的节点。一致性哈希提供的好处是，当节点数量出现变化，例如节点宕机或者新节点加入的时候，整体 key 和节点的映射关系不会出现大的变化。 singleflightsingleflight 的功能是当多个并发请求同时请求相同的 key 的时候，保证函数只调用一次。singleflight 本身也是常见的缓存击穿的问题的解决方案，可以避免大量同样的请求到底层数据库，起到对数据库的保护作用。 singleflight 的实现原理是在调用请求的时候手动分配一个 key。在请求调用之前，会先查询这个 key 对应的请求是否已经存在了，如果还不存在，就 new 一个 key 对应的请求；如果已经存在，就进入等待流程。当请求调用函数获取到数据之后，会通知所有在等待的请求，操作完成，并在 map 中删除这个 key。其他在等待的请求收到通知后会获取数据并返回。 实际使用中，考虑到调用底层的查询可能出错或者超时，一般还要分配一个超时时间或者主动让 key 对应的请求失效，这里暂时不讨论 singleflight 的实现细节。 lruLRU 算法（最近最少使用）是比较常用的缓存更新算法。groupcache 使用的 LRU 算法并没有特殊的优化，将最新用到的数据放到链表的头部，淘汰的时候从链表尾部开始淘汰。 这种常规 LRU 算法的问题是如果出现扫描式大量数据被访问的时候，会导致整个缓存的数据都被淘汰，如果后续这些数据不再被用到，会导致缓存命中率大大降低。其他一些应用，例如 MySQL 通过把 LRU 的链表分成 old 和 young 两部分来缓解这个问题。 httppoolhttppool 封装了和各个节点的通信过程，并保存了所有远端节点的通信地址。当 groupcache 需要向其他节点发起 http 请求获取数据的时候，就会通过 httppool 调用对应节点的提供的 httpGetter 方法，从指定的服务器节点获取数据。 参考资料 groupcache 官方文档 groupcache github 地址","link":"/2024/04/28/groupcache%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/"},{"title":"singleflight简单介绍","text":"简单介绍singleflight 包提供了一个抑制重复函数调用的机制。 singleflight 主要用于解决缓存击穿的问题。缓存击穿指的是热点数据不在缓存之中，表现为数据库收到大量查询请求，压力突增，性能下降。 singleflight 可以把相同的并发请求挂起，只保留一个请求去请求底层的数据库。当请求到结果的时候，给所有的并发请求返回同样的结果。 基本流程Demosingleflight 的使用非常简单。singleflight 判断是否为同样请求的方法是调用内部函数时传入的 key 是否相同。 下面的 Demo 模拟了 10 个同样的并发请求调用的场景。 1234567891011121314151617181920const key = &quot;same request&quot;func singleflightDemo() { var wg sync.WaitGroup sfGroup := &amp;singleflight.Group{} for i := 0; i &lt; 10; i++ { wg.Add(1) go func(idx int) { defer wg.Done() val, err, _ := sfGroup.Do(key, func() (interface{}, error) { fmt.Printf(&quot;singleflight idx:%d, key:%s\\n&quot;, idx, key) // 添加 sleep 可以延长单个请求的时间，保证其他请求都等到该请求结束才拿到结果 time.Sleep(10 * time.Millisecond) return &quot;value&quot;, nil }) fmt.Printf(&quot;idx:%d, val:%v, err:%v\\n&quot;, idx, val, err) }(i) } wg.Wait()} 执行的结果如下： 1234567891011singleflight idx:9, key:same requestidx:2, val:value, err:&lt;nil&gt;idx:6, val:value, err:&lt;nil&gt;idx:8, val:value, err:&lt;nil&gt;idx:5, val:value, err:&lt;nil&gt;idx:7, val:value, err:&lt;nil&gt;idx:1, val:value, err:&lt;nil&gt;idx:3, val:value, err:&lt;nil&gt;idx:9, val:value, err:&lt;nil&gt;idx:4, val:value, err:&lt;nil&gt;idx:0, val:value, err:&lt;nil&gt; 只有某一个协程请求到了内部函数，输出 singleflight idx...，其他的协程不会调用到内部函数。 实现流程singleflight 主要通过 sync. Mutex 和 sync. WaitGroup 来实现并发控制功能。 Group 结构体和 call 结构体如下所示： 123456789101112131415161718192021// Group represents a class of work and forms a namespace in// which units of work can be executed with duplicate suppression. type Group struct { mu sync.Mutex // protects m m map[string]*call // lazily initialized }// call is an in-flight or completed singleflight.Do call type call struct { wg sync.WaitGroup // These fields are written once before the WaitGroup is done // and are only read after the WaitGroup is done. val interface{} err error // These fields are read and written with the singleflight // mutex held before the WaitGroup is done, and are read but // not written after the WaitGroup is done. dups int chans []chan&lt;- Result } 简单来说，singleflight 抑制并发请求的流程如下： 当请求来到 singleflight 的时候，首先检查 m 中是否存在对应的 key 如果存在 说明已经有函数正在请求底层数据库（即有 call 结构） 调用 Wait 方法，等待对应 call 的 WaitGroup 执行完成 如果不存在 说明这是首次调用，封装一个新的 call 在 m 中插入对应的 key，执行底层函数 执行完毕之后，把结果保存在 call 结构体中，调用 wg.Done 方法 其他调用从 call 中取出调用结果 局限性 Do 方法是一个同步调用的方法，下游出现超时会导致服务阻塞 解决方法是改用 DoChan 方法，并增加 select 来实现超时控制，保证在指定时间内可以拿到返回数据或者超时报错。 某次调用恰好出现了错误或者阻塞，会导致所有调用都得到同样的错误 可以通过 Forget 方法，定时删除 singleflight 的 Group 结构中记忆的 key，让其他的请求可以自行请求下游服务，开启一个新的 call 下游调用。 组件分析Do1234567891011121314151617181920212223242526272829// Do executes and returns the results of the given function, making// sure that only one execution is in-flight for a given key at a // time. If a duplicate comes in, the duplicate caller waits for the // original to complete and receives the same results. // The return value shared indicates whether v was given to multiple callers. func (g *Group) Do(key string, fn func() (interface{}, error)) (v interface{}, err error, shared bool) { g.mu.Lock() if g.m == nil { g.m = make(map[string]*call) } if c, ok := g.m[key]; ok { c.dups++ g.mu.Unlock() c.wg.Wait() if e, ok := c.err.(*panicError); ok { panic(e) } else if c.err == errGoexit { runtime.Goexit() } return c.val, c.err, true } c := new(call) c.wg.Add(1) g.m[key] = c g.mu.Unlock() g.doCall(c, key, fn) return c.val, c.err, c.dups &gt; 0 } Do 方法是 singleflight 最主要的调用方式，在上面的 Demo 里面也已经看到了使用的方法。 Do 方法本身的逻辑非常简单，和上面的实现流程保持一致： 如果当前 key 是第一个发起请求的协程，就新建一个 call 结构体，调用 doCall 方法来执行用户传入的 fn 函数 如果当前 key 不是第一个发起请求的协程，就调用 wg.Wait() 方法，等待调用完毕之后，从 call 结构体中获取调用结果 DoChan123456789101112131415161718192021222324// DoChan is like Do but returns a channel that will receive the// results when they are ready. // // The returned channel will not be closed. func (g *Group) DoChan(key string, fn func() (interface{}, error)) &lt;-chan Result { ch := make(chan Result, 1) g.mu.Lock() if g.m == nil { g.m = make(map[string]*call) } if c, ok := g.m[key]; ok { c.dups++ c.chans = append(c.chans, ch) g.mu.Unlock() return ch } c := &amp;call{chans: []chan&lt;- Result{ch}} c.wg.Add(1) g.m[key] = c g.mu.Unlock() go g.doCall(c, key, fn) return ch } DoChan 方法和 Do 方法基本一致，区别在于 DoChan 是异步的版本，返回一个 chan 结构体，由用户自己等待来进行消费。 Forget123456// Forget tells the singleflight to forget about a key. Future calls// to Do for this key will call the function rather than waiting for// an earlier call to complete. func (g *Group) Forget(key string) { g.mu.Lock() delete(g.m, key) g.mu.Unlock() } Forget 方法本身很短，就是把 Group 结构体中 m 对应的 key 删去，后续有同样的 key 的请求到来的时候，可以开启一个新的 call 调用下游函数。 doCall123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263// doCall handles the single call for a key.func (g *Group) doCall(c *call, key string, fn func() (interface{}, error)) { normalReturn := false recovered := false // use double-defer to distinguish panic from runtime.Goexit, // more details see https://golang.org/cl/134395 defer func() { // the given function invoked runtime.Goexit if !normalReturn &amp;&amp; !recovered { c.err = errGoexit } g.mu.Lock() defer g.mu.Unlock() c.wg.Done() if g.m[key] == c { delete(g.m, key) } if e, ok := c.err.(*panicError); ok { // In order to prevent the waiting channels from being blocked forever, // needs to ensure that this panic cannot be recovered. if len(c.chans) &gt; 0 { go panic(e) select {} // Keep this goroutine around so that it will appear in the crash dump. } else { panic(e) } } else if c.err == errGoexit { // Already in the process of goexit, no need to call again } else { // Normal return for _, ch := range c.chans { ch &lt;- Result{c.val, c.err, c.dups &gt; 0} } } }() func() { defer func() { if !normalReturn { // Ideally, we would wait to take a stack trace until we've determined // whether this is a panic or a runtime.Goexit. // // Unfortunately, the only way we can distinguish the two is to see // whether the recover stopped the goroutine from terminating, and by // the time we know that, the part of the stack trace relevant to the // panic has been discarded. if r := recover(); r != nil { c.err = newPanicError(r) } } }() c.val, c.err = fn() normalReturn = true }() if !normalReturn { recovered = true } } doCall 函数会比较长，主要是为了保证即使 fn 函数出现了 panic 或者 runtime.Exit 之后的处理方式。 在同步的情况下，出错之后可以在 Do 中进行统一处理；而在异步的情况下，处理被交给了业务方，所以 doCall 中直接引发 panic。 WaitGroup 的使用细节整体代码中有一个有趣的细节：一般 WaitGroup 的使用方式是主线程执行 Wait 方法，多个并发的线程在完成之后执行 Done 方法，主线程等待多个并发线程执行完毕。 而在 Do 和 doCall 方法中，可以第一个执行的线程会执行 Done 方法，而多个并发线程则执行 Wait 来等待第一个执行线程的执行完毕。 参考资料 singleflight 官方文档 深入理解Golang并发工具-Singleflight","link":"/2024/06/24/singleflight%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/"}],"tags":[{"name":"工程备忘","slug":"工程备忘","link":"/tags/%E5%B7%A5%E7%A8%8B%E5%A4%87%E5%BF%98/"},{"name":"技术基础","slug":"技术基础","link":"/tags/%E6%8A%80%E6%9C%AF%E5%9F%BA%E7%A1%80/"},{"name":"消息队列","slug":"消息队列","link":"/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"数据存储","slug":"数据存储","link":"/tags/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/"},{"name":"分布式架构","slug":"分布式架构","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"组件分析","slug":"组件分析","link":"/tags/%E7%BB%84%E4%BB%B6%E5%88%86%E6%9E%90/"}],"categories":[]}